{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063055a5",
   "metadata": {},
   "source": [
    "# Notebook for checking the data format on aisemgnet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448afa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################\n",
    "#\n",
    "# Copyright (C) 2021-2023 Maxim Integrated Products, Inc. All Rights Reserved.\n",
    "# Copyright (C) 2024 Analog Devices, Inc. All Rights Reserved.\n",
    "#\n",
    "# This software is proprietary to Analog Devices, Inc. and its licensors.\n",
    "###################################################################################################\n",
    "\"\"\"\n",
    "Classes and functions used to create AISegment dataset.\n",
    "\"\"\"\n",
    "\n",
    "import errno\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import ai8x\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "class AISegment(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    AISegment Human Portrait Matting Dataset\n",
    "    (https://www.kaggle.com/laurentmih/AISegmentcom-matting-human-datasets/).\n",
    "    The image files are in RGB format and corresponding portrait matting files are in RGBA\n",
    "    format where the alpha channel is 0 or 255 for background and portrait respectively.\n",
    "\n",
    "    Available classes: Background, Portrait\n",
    "\n",
    "    If the memory based approach is selected, 20,000 images are used and a single square cropped\n",
    "    image is generated per source image.\n",
    "    If the disk based approach is selected, `num_of_cropped_img` images are cropped from\n",
    "    the original image to generate square images.\n",
    "    The cropped image(s) are then downsampled and corresponding\n",
    "    binary labels are generated (see the `__gen_dataset()` function for details)\n",
    "\n",
    "    `im_size` determines the image resolution for dowsampled square images\n",
    "    and can be specified as (80, 80) or (352, 352).\n",
    "\n",
    "    The code separates available dataset items into test and train sets using `train_ratio`.\n",
    "\n",
    "    For the low resolution (80, 80) version, the data loader uses a memory based approach, whereas\n",
    "    for the high resolution (352, 352) version, a memory or disk based approach is employed\n",
    "    selected by the `use_memory` initialization parameter. If both memory based approach and high\n",
    "    resolution are selected, `num_of_imgs_to_use_hr` images are processed in order to limit\n",
    "    memory consumption.\n",
    "    \"\"\"\n",
    "\n",
    "    org_img_dim = [800, 600]\n",
    "    img_crp_dim = [600, 600]\n",
    "    num_of_cropped_imgs = 3\n",
    "\n",
    "    train_ratio = 0.9\n",
    "\n",
    "    num_of_imgs_to_use_hr = 20000\n",
    "\n",
    "    def __init__(self, root_dir, d_type, transform=None, im_size=(80, 80), use_memory=False,\n",
    "                 truncate_testset=False):\n",
    "\n",
    "        if im_size not in ((80, 80), (352, 352)):\n",
    "            raise ValueError('im_size can only be set to (80, 80) or (352, 352)')\n",
    "\n",
    "        if d_type not in ('test', 'train'):\n",
    "            raise ValueError(\"d_type can only be set to 'test' or 'train'\")\n",
    "\n",
    "        if im_size == (80, 80) and not use_memory:\n",
    "            raise ValueError(\"use_memory can only be set to True for low resolution option: \"\n",
    "                             \"'im_size = (80, 80)'\")\n",
    "\n",
    "        raw_img_folder = os.path.join(root_dir, self.__class__.__name__, 'clip_img')\n",
    "        raw_matting_folder = os.path.join(root_dir, self.__class__.__name__, 'matting')\n",
    "\n",
    "        if not os.path.exists(raw_img_folder) or not os.path.exists(raw_matting_folder):\n",
    "            print('\\nDownload the archive file from: '\n",
    "                  'https://www.kaggle.com/laurentmih/AISegmentcom-matting-human-datasets/')\n",
    "            print('Extract the downloaded archive to path [data_dir]/AISegment.')\n",
    "            print('The download process may require additional authentication.')\n",
    "\n",
    "            sys.exit()\n",
    "\n",
    "        self.d_type = d_type\n",
    "\n",
    "        self.is_truncated = False\n",
    "\n",
    "        vertical_crop_area = AISegment.org_img_dim[0] - AISegment.img_crp_dim[0]\n",
    "\n",
    "        if vertical_crop_area % (AISegment.num_of_cropped_imgs - 1) != 0:\n",
    "            raise ValueError('The number of cropped images should be set such that the first '\n",
    "                             'image is fit to top and the last one to bottom.')\n",
    "\n",
    "        self.transform = transform\n",
    "        self.img_ds_dim = im_size\n",
    "        self.is_memory_based_approach_in_use = use_memory\n",
    "        self.is_high_res_in_use = self.img_ds_dim == (352, 352)\n",
    "\n",
    "        # Path management section:\n",
    "        # Paths generated for:\n",
    "        # 1) Processed train/test folders: pickle images for each file will be stored under if disk\n",
    "        #    based approach is selected\n",
    "        # 2) Two pickle files for dataset information dataframes for test and train\n",
    "        # 3) Two pickle files for storing all data in a single file for test and train if memory\n",
    "        #    based approach is selected\n",
    "        resolution_str = f'{im_size[0]}x{im_size[1]}'\n",
    "\n",
    "        self.processed_train_data_folder = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__,\n",
    "                         'processed_train_' + resolution_str + '_disk')\n",
    "\n",
    "        self.processed_test_data_folder = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__,\n",
    "                         'processed_test_' + resolution_str + '_disk')\n",
    "\n",
    "        train_dataset_info_file_path = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__, 'train_datafiles_info.pkl')\n",
    "\n",
    "        test_dataset_info_file_path = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__, 'test_datafiles_info.pkl')\n",
    "\n",
    "        # If memory based approach is in use, processed files will be written into single pickle\n",
    "        # file per training/test\n",
    "        train_dataset_pkl_file_path = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__,\n",
    "                         f'train_set_{im_size[0]}x{im_size[1]}.pkl')\n",
    "        test_dataset_pkl_file_path = \\\n",
    "            os.path.join(root_dir, self.__class__.__name__,\n",
    "                         f'test_set_{im_size[0]}x{im_size[1]}.pkl')\n",
    "\n",
    "        # These image and label lists will only be in use when memory based approach is selected\n",
    "        self.img_list = []\n",
    "        self.lbl_list = []\n",
    "\n",
    "        # Generate dataset information files - valid for both memory and disk based approaches:\n",
    "        if not os.path.isfile(train_dataset_info_file_path) or \\\n",
    "           not os.path.isfile(test_dataset_info_file_path):\n",
    "\n",
    "            print('Creating dataset information files...')\n",
    "\n",
    "            train_img_files_info = pd.DataFrame()\n",
    "            test_img_files_info = pd.DataFrame()\n",
    "\n",
    "            i = 0\n",
    "            j = 0\n",
    "            for (root, _, files) in os.walk(raw_img_folder):\n",
    "                for file_name in files:\n",
    "\n",
    "                    # Construct image file path\n",
    "                    img_file_path = os.path.join(root, file_name)\n",
    "\n",
    "                    # Generate corresponding matting file path (label file)\n",
    "                    matting_file_path = img_file_path.replace(raw_img_folder, raw_matting_folder)\n",
    "                    matting_file_path = matting_file_path.replace('clip_', 'matting_')\n",
    "                    matting_file_path = matting_file_path.replace('.jpg', '.png')\n",
    "\n",
    "                    # Determine training or test dataset\n",
    "                    img_name = os.path.splitext(os.path.basename(file_name))[0]\n",
    "\n",
    "                    # Keep crop indexes and place all cropped image/s in the same test/train set\n",
    "                    if hash(img_name) % 10 < 10 * AISegment.train_ratio:\n",
    "                        for img_crop_idx in range(AISegment.num_of_cropped_imgs):\n",
    "                            train_img_files_info.loc[i, 'img_file_path'] = img_file_path\n",
    "                            train_img_files_info.loc[i, 'lbl_file_path'] = matting_file_path\n",
    "                            train_img_files_info.loc[i, 'pickle_file_name'] = \\\n",
    "                                img_name + f'_{img_crop_idx}.pkl'\n",
    "                            train_img_files_info.loc[i, 'crp_idx'] = img_crop_idx\n",
    "                            i = i + 1\n",
    "                    else:\n",
    "                        for img_crop_idx in range(AISegment.num_of_cropped_imgs):\n",
    "                            test_img_files_info.loc[j, 'img_file_path'] = img_file_path\n",
    "                            test_img_files_info.loc[j, 'lbl_file_path'] = matting_file_path\n",
    "                            test_img_files_info.loc[j, 'pickle_file_name'] = \\\n",
    "                                img_name + f'_{img_crop_idx}.pkl'\n",
    "                            test_img_files_info.loc[j, 'crp_idx'] = img_crop_idx\n",
    "                            j = j + 1\n",
    "\n",
    "            # Save training and test dataset file information data frames to disk\n",
    "            train_img_files_info.to_pickle(train_dataset_info_file_path)\n",
    "            test_img_files_info.to_pickle(test_dataset_info_file_path)\n",
    "            print('Created dataset information files...')\n",
    "\n",
    "        else:\n",
    "            print('Dataset information file exists and reused...')\n",
    "            train_img_files_info = pd.read_pickle(train_dataset_info_file_path)\n",
    "            test_img_files_info = pd.read_pickle(test_dataset_info_file_path)\n",
    "\n",
    "        # Select dataset information file name,\n",
    "        # processed dataset file name which will be used when memory based approach is in use and\n",
    "        # processed folder path that will store pkl files when disk based approach is in use.\n",
    "        if self.d_type == 'train':\n",
    "            self.img_files_info = train_img_files_info\n",
    "            self.dataset_pkl_file_path = train_dataset_pkl_file_path\n",
    "            self.processed_folder_path = self.processed_train_data_folder\n",
    "\n",
    "        elif self.d_type == 'test':\n",
    "            self.img_files_info = test_img_files_info\n",
    "            self.dataset_pkl_file_path = test_dataset_pkl_file_path\n",
    "            self.processed_folder_path = self.processed_test_data_folder\n",
    "            if truncate_testset:\n",
    "                self.is_truncated = True\n",
    "\n",
    "        else:\n",
    "            print(f'Unknown data type: {self.d_type}')\n",
    "            return\n",
    "\n",
    "        self.__create_pkl_files()\n",
    "\n",
    "    def __create_pkl_files(self):\n",
    "        if self.__check_pkl_files_exist():\n",
    "            print('\\nPickle files of images are already generated...\\n')\n",
    "\n",
    "            if self.is_memory_based_approach_in_use:\n",
    "                (self.img_list, self.lbl_list) = \\\n",
    "                    pickle.load(open(self.dataset_pkl_file_path, 'rb'))\n",
    "            return\n",
    "\n",
    "        if not self.is_memory_based_approach_in_use:\n",
    "            self.__makedir_exist_ok(self.processed_train_data_folder)\n",
    "            self.__makedir_exist_ok(self.processed_test_data_folder)\n",
    "\n",
    "        self.__gen_datasets()\n",
    "\n",
    "    @staticmethod\n",
    "    def __makedir_exist_ok(dirpath):\n",
    "        try:\n",
    "            os.makedirs(dirpath)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    def __check_pkl_files_exist(self):\n",
    "        if self.is_memory_based_approach_in_use:\n",
    "            return os.path.exists(self.dataset_pkl_file_path)\n",
    "\n",
    "        if os.path.exists(self.processed_folder_path) and \\\n",
    "           os.path.isdir(self.processed_folder_path):\n",
    "            pkl_files = [f for f in sorted(os.listdir(self.processed_folder_path))\n",
    "                         if f.endswith('.pkl')]\n",
    "        else:\n",
    "            pkl_files = []\n",
    "        return len(pkl_files) > 0\n",
    "\n",
    "    @staticmethod\n",
    "    def __normalize_image(image):\n",
    "        return image / 256\n",
    "\n",
    "    @classmethod\n",
    "    def crop_image_and_label(cls, img, lbl_rgba, img_crp_idx):\n",
    "\n",
    "        \"\"\"Crops square image from the original image crp_idx determines the crop area\"\"\"\n",
    "\n",
    "        vertical_crop_area = cls.org_img_dim[0] - cls.img_crp_dim[0]\n",
    "        step_size = vertical_crop_area / (cls.num_of_cropped_imgs - 1)\n",
    "\n",
    "        # Determine top left coordinate of the crop area\n",
    "        top_left_x = 0\n",
    "        top_left_y = 0 + img_crp_idx * step_size\n",
    "\n",
    "        # Determine bottom right coordinate of the crop area\n",
    "        bottom_right_x = cls.img_crp_dim[0]\n",
    "        bottom_right_y = top_left_y + cls.img_crp_dim[0]\n",
    "\n",
    "        img_crp = img.crop((top_left_x, top_left_y, bottom_right_x, bottom_right_y))\n",
    "        img_crp_lbl = lbl_rgba.crop((top_left_x, top_left_y, bottom_right_x, bottom_right_y))\n",
    "\n",
    "        return (img_crp, img_crp_lbl)\n",
    "\n",
    "    def __gen_datasets(self):\n",
    "        print('\\nGenerating dataset pickle file(s) from the raw data files...\\n')\n",
    "\n",
    "        total_num_of_processed_files = 0\n",
    "        for _, row in self.img_files_info.iterrows():\n",
    "\n",
    "            img_file = row['img_file_path']\n",
    "            matting_file = row['lbl_file_path']\n",
    "            pickle_file = row['pickle_file_name']\n",
    "            img_crp_idx = row['crp_idx']\n",
    "\n",
    "            pickle_file = os.path.join(self.processed_folder_path, row['pickle_file_name'])\n",
    "\n",
    "            img = Image.open(img_file)\n",
    "            lbl_rgba = Image.open(matting_file)\n",
    "\n",
    "            (img_crp, img_crp_lbl) = self.crop_image_and_label(img, lbl_rgba, img_crp_idx)\n",
    "\n",
    "            # Resize and typecast before saving: 8 bits is enough for RGB values\n",
    "            img_crp = img_crp.resize(self.img_ds_dim)\n",
    "            img_crp = np.asarray(img_crp).astype(np.uint8)\n",
    "\n",
    "            # Resize and typecast before saving: 8 bits is enough for label values\n",
    "            # (as bool is also 8 bits, uint8 is selected).\n",
    "            # As label image is 4 channel RGBA, binary label is generated using the last channel.\n",
    "            img_crp_lbl = img_crp_lbl.resize(self.img_ds_dim)\n",
    "            img_crp_lbl = (np.asarray(img_crp_lbl)[:, :, 3] == 0).astype(np.uint8)\n",
    "\n",
    "            if self.is_memory_based_approach_in_use:\n",
    "\n",
    "                if not self.is_high_res_in_use:\n",
    "                    self.img_list.append(img_crp)\n",
    "                    self.lbl_list.append(img_crp_lbl)\n",
    "                else:\n",
    "                    # For the memory based approach and for high resolution images, not all cropped\n",
    "                    # images are moved to test or training set. Instead, only a randomly selected\n",
    "                    # image is used to sample more different images\n",
    "                    if img_crp_idx == 0:\n",
    "                        # Generate random number for the cropped images batch at the start\n",
    "                        idx_to_add = np.random.choice(AISegment.num_of_cropped_imgs, 1)\n",
    "\n",
    "                    if img_crp_idx == idx_to_add:\n",
    "                        self.img_list.append(img_crp)\n",
    "                        self.lbl_list.append(img_crp_lbl)\n",
    "\n",
    "                        # For the memory based approach, and for high resolution images, not all\n",
    "                        # images are processed, only the first `num_of_imgs_to_use_hr`\n",
    "                        if len(self.img_list) == AISegment.num_of_imgs_to_use_hr:\n",
    "                            break\n",
    "            else:\n",
    "                # Save each image and label to disk in disk based approach:\n",
    "                pickle.dump((img_crp, img_crp_lbl), open(pickle_file, 'wb'))\n",
    "\n",
    "            total_num_of_processed_files = total_num_of_processed_files + 1\n",
    "\n",
    "        # Save pickle files in memory based approach\n",
    "        if self.is_memory_based_approach_in_use:\n",
    "            pickle.dump((self.img_list, self.lbl_list), open(self.dataset_pkl_file_path, 'wb'))\n",
    "\n",
    "        print(f'\\nTotal number of processed files: {total_num_of_processed_files}\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.is_truncated:\n",
    "            return 1\n",
    "        if self.is_memory_based_approach_in_use:\n",
    "            return len(self.img_list)\n",
    "        return self.img_files_info.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index >= len(self):\n",
    "            raise IndexError\n",
    "\n",
    "        if self.is_truncated:\n",
    "            index = 0\n",
    "\n",
    "        if self.is_memory_based_approach_in_use:\n",
    "            img = self.img_list[index]\n",
    "            lbl = self.lbl_list[index]\n",
    "        else:\n",
    "            # Read pickle file\n",
    "            pickle_file = os.path.join(self.processed_folder_path,\n",
    "                                       self.img_files_info.loc[index, 'pickle_file_name'])\n",
    "            (img, lbl) = pickle.load(open(pickle_file, 'rb'))\n",
    "\n",
    "        img = self.__normalize_image(img).astype(np.float32)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, lbl.astype(np.int64)\n",
    "\n",
    "\n",
    "def AISegment_get_datasets(data, load_train=True, load_test=True, im_size=(80, 80),\n",
    "                           fold_ratio=1, use_memory=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the AISegment dataset.\n",
    "\n",
    "    The dataset includes matting files of each image: alpha pixel value 0 for background and\n",
    "    RGB value for portrait.\n",
    "\n",
    "    As the AISegment dataset does not have explicit test/train separation, the dataset\n",
    "    is split into training + validation and test sets using given ratio (90:10 by default).\n",
    "\n",
    "    Images will have `im_size` resolution and the data loader will use memory or disk as set by\n",
    "    `use_memory`.\n",
    "    \"\"\"\n",
    "    (data_dir, args) = data\n",
    "\n",
    "    if load_train:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ai8x.normalize(args=args),\n",
    "            ai8x.fold(fold_ratio=fold_ratio)\n",
    "        ])\n",
    "\n",
    "        train_dataset = AISegment(root_dir=data_dir, d_type='train',\n",
    "                                  transform=train_transform,\n",
    "                                  im_size=im_size, use_memory=use_memory,\n",
    "                                  truncate_testset=False)\n",
    "        print(f'Train dataset length: {len(train_dataset)}\\n')\n",
    "    else:\n",
    "        train_dataset = None\n",
    "\n",
    "    if load_test:\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            ai8x.normalize(args=args),\n",
    "            ai8x.fold(fold_ratio=fold_ratio)\n",
    "        ])\n",
    "\n",
    "        test_dataset = AISegment(root_dir=data_dir, d_type='test',\n",
    "                                 transform=test_transform,\n",
    "                                 im_size=im_size, use_memory=use_memory,\n",
    "                                 truncate_testset=args.truncate_testset)\n",
    "\n",
    "        print(f'Test dataset length: {len(test_dataset)}\\n')\n",
    "    else:\n",
    "        test_dataset = None\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def AISegment80_get_datasets(data, load_train=True, load_test=True):\n",
    "    \"\"\"Returns AISegment datasets of low resolution (80x80)\"\"\"\n",
    "    return AISegment_get_datasets(data, load_train, load_test, im_size=(80, 80), fold_ratio=1,\n",
    "                                  use_memory=True)\n",
    "\n",
    "\n",
    "def AISegment352_get_datasets_use_disk(data, load_train=True, load_test=True):\n",
    "    \"\"\"Returns AISegment datasets of high resolution (352x352) and uses disk based approach\"\"\"\n",
    "    return AISegment_get_datasets(data, load_train, load_test, im_size=(352, 352), fold_ratio=4,\n",
    "                                  use_memory=False)\n",
    "\n",
    "\n",
    "def AISegment352_get_datasets_use_memory(data, load_train=True, load_test=True):\n",
    "    \"\"\"Returns AISegment datasets of high resolution (352x352) and uses memory based approach\"\"\"\n",
    "    return AISegment_get_datasets(data, load_train, load_test, im_size=(352, 352), fold_ratio=4,\n",
    "                                  use_memory=True)\n",
    "\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        'name': 'AISegment_80',\n",
    "        'input': (3, 80, 80),\n",
    "        'output': (0, 1),\n",
    "        'loader': AISegment80_get_datasets,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AISegment_352',\n",
    "        'input': (48, 88, 88),\n",
    "        'output': (0, 1),\n",
    "        'loader': AISegment352_get_datasets_use_disk,\n",
    "        'fold_ratio': 4,\n",
    "    },\n",
    "    {\n",
    "        'name': 'AISegment_352_memory',\n",
    "        'input': (48, 88, 88),\n",
    "        'output': (0, 1),\n",
    "        'loader': AISegment352_get_datasets_use_memory,\n",
    "        'fold_ratio': 4,\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca0456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(act_mode_8bit=False)\n",
    "train_dataset, test_dataset = AISegment80_get_datasets((\"../dataset\", args), load_train=False, load_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43558aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1\n",
    "\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10,24))\n",
    "\n",
    "for i in range(0, num_examples):\n",
    "    image, mask = train_dataset.__getitem__(i)\n",
    "\n",
    "    print(\"image/mask type:        \", type(image), type(mask))\n",
    "    print(\"image/mask shape:       \", image.shape, mask.shape)\n",
    "    print(\"image/mask datatype:    \", image.dtype, mask.dtype)\n",
    "    print(\"image min and max value:\", image.min(), image.max())\n",
    "    print(\"mask min and max value: \" , mask.min(), mask.max())\n",
    " \n",
    "    plt.subplot(num_examples,2,2*i+1)\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.subplot(num_examples,2,2*i+2)\n",
    "    plt.imshow(mask)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
