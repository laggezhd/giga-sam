{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620ebb31",
   "metadata": {},
   "source": [
    "# MinimalSAM Training\n",
    "\n",
    "This notebook contains the training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfd57c",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "⚠️ Make sure to set `using_colab=True` if running on Google Colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM2 is already installed.\n",
      "BASE_PATH: /home/sschwartz/Dokumente/ml_mcu_hs_25/giga-sam\n",
      "PyTorch version:     2.9.1+cu128\n",
      "Torchvision version: 0.24.1+cu128\n",
      "CUDA is available:   False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "using_colab=False\n",
    "\n",
    "# check if minimal_sam and sam2 are installed\n",
    "try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    print(\"SAM2 is already installed.\")\n",
    "except:   \n",
    "    !pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "try:\n",
    "    from minimal_sam.models import MinimalSAM\n",
    "    print(\"MinimalSAM is already installed.\")\n",
    "except:\n",
    "    !pip install 'git+https://github.com/laggezhd/giga-sam.git'\n",
    "\n",
    "# setup environment\n",
    "if using_colab:\n",
    "    !git clone https://github.com/facebookresearch/sam2.git\n",
    "    !git clone https://github.com/laggezhd/giga-sam.git\n",
    "    !cd giga-sam/checkpoints && ./download_ckpts.sh\n",
    "    !cd giga-sam/dataset && ./download_dataset.sh\n",
    "\n",
    "    os.environ[\"BASE_PATH\"] = os.getcwd()\n",
    "    os.environ[\"ANNS_PATH\"] = \"/content/drive/MyDrive/dataset/\"\n",
    "else:\n",
    "    os.environ[\"BASE_PATH\"] = os.path.dirname(os.getcwd())\n",
    "    os.environ[\"ANNS_PATH\"] = os.path.join(os.environ[\"BASE_PATH\"], \"configs/annotations\")\n",
    "\n",
    "print(f\"BASE_PATH: {os.environ['BASE_PATH']}\")\n",
    "print(f\"ANNS_PATH: {os.environ['ANNS_PATH']}\")\n",
    "print(f\"PyTorch version:     {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"CUDA is available:   {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pathlib import Path\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "from minimal_sam.models import MicroSAM, MinimalSAM, PicoSAM\n",
    "from minimal_sam.utils.data import MinimalSamDataset\n",
    "from minimal_sam.utils.loss import bce_dice_loss\n",
    "from minimal_sam.utils.metrics import compute_iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f61f95",
   "metadata": {},
   "source": [
    "requires manual authentication for both Google Drive and WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a05f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5e82f7",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"MicroSAM\"  # choose between \"MicroSAM\", \"PicoSAM\" and \"MinimalSAM\"\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# === Files and Directories ===\n",
    "COCO_ANN_FILE  = Path(os.environ[\"BASE_PATH\"]).joinpath(\"giga-sam/dataset/annotations/instances_train2017.json\")\n",
    "COCO_FILTERED  = Path(os.environ[\"ANNS_PATH\"]).joinpath(f\"filtered_anns_{IMG_SIZE}x{IMG_SIZE}.json\")\n",
    "\n",
    "OUTPUT_DIR  = Path(os.environ[\"BASE_PATH\"]).joinpath(\"outputs\")\n",
    "DATASET_DIR = Path(os.environ[\"BASE_PATH\"]).joinpath(\"giga-sam/dataset\")\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "assert COCO_ANN_FILE.exists(), f\"COCO annotation file not found at {COCO_ANN_FILE}\"\n",
    "assert COCO_FILTERED.exists(), f\"Filtered COCO annotations not found at {COCO_FILTERED}\"\n",
    "\n",
    "# === Training Hyperparameters ===\n",
    "NUM_WORKERS = 12  # speeds up training!\n",
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 3e-4\n",
    "MAX_LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "def train():\n",
    "\n",
    "    # Set training device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Training on: {device}\")\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"MLonMCU\",\n",
    "        name=MODEL,\n",
    "        config={\n",
    "            \"img_size\": IMG_SIZE,\n",
    "            \"epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"max_lr\": MAX_LR,\n",
    "            \"weight_decay\": WEIGHT_DECAY\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    match MODEL:\n",
    "        case \"MicroSAM\":\n",
    "            model = MicroSAM().to(device)\n",
    "        case \"PicoSAM\":\n",
    "            model = PicoSAM().to(device)\n",
    "        case \"MinimalSAM\":\n",
    "            model = MinimalSAM().to(device)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown model type: {MODEL}! Choose between 'MicroSAM', 'PicoSAM' and 'MinimalSAM'.\")\n",
    "\n",
    "    # Initialize dataset and dataloaders\n",
    "    dataset = MinimalSamDataset(IMG_SIZE, DATASET_DIR, COCO_ANN_FILE, COCO_FILTERED)\n",
    "\n",
    "    train_len = int(len(dataset) * 0.95)\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, len(dataset) - train_len])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=MAX_LR, steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS)\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "    best_val_iou = 0.0\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "\n",
    "        total_loss, total_iou, samples = 0, 0, 0  # logging params\n",
    "\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1} - Train\")):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                logits = model(images)\n",
    "                loss = bce_dice_loss(logits, masks)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            scaler.unscale_(optimizer)  # unscale gradients before clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)  # gradient clipping\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # logging\n",
    "            batch_iou = compute_iou(logits, masks)\n",
    "\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_iou += batch_iou * images.size(0)\n",
    "            samples += images.size(0)\n",
    "\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_mIoU\": batch_iou,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss\": total_loss / samples,\n",
    "            \"train_mIoU\": total_iou / samples,\n",
    "            \"lr\": scheduler.get_last_lr()[0],\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        # === Validation Loop ===\n",
    "        model.eval()\n",
    "        val_loss, val_iou, val_samples = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch + 1} - Val\"):\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "                logits = model(images)\n",
    "                loss = bce_dice_loss(logits, masks)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_iou += compute_iou(logits, masks) * images.size(0)\n",
    "                val_samples += images.size(0)\n",
    "\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_loss / val_samples,\n",
    "            \"val_mIoU\": val_iou / val_samples,\n",
    "            \"epoch\": epoch + 1\n",
    "        })\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"{MODEL}_epoch{epoch + 1}.pt\"))\n",
    "\n",
    "        # Save best model\n",
    "        avg_val_iou = val_iou / val_samples\n",
    "\n",
    "        if avg_val_iou > best_val_iou:\n",
    "            best_val_iou = avg_val_iou\n",
    "            best_model_path = os.path.join(OUTPUT_DIR, f\"{MODEL}_best.pt\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            wandb.save(best_model_path)\n",
    "            print(f\"New best model saved with mIoU: {best_val_iou:.4f}\")\n",
    "\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "giga-sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
